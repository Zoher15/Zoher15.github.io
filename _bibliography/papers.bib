---
---
@misc{kachwala2025taskalignedpromptingimproveszeroshot,
  title={Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models}, 
  author={Zoher Kachwala and Danishjeet Singh and Danielle Yang and Filippo Menczer},
  year={2025},
  eprint={2506.11031},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2506.11031},
  code={https://github.com/Zoher15/Zero-shot-s2},
  status={Under Review at NeurIPS 2025},
  preview={task_aligned.png},
  selected={true},
  bibtex_show={true},
  pdf={Task_aligned_prompting_improves_zero_shot_detection_of_AI_generated_images_by_Vision_Language_Models.pdf},
  abstract={As image generators produce increasingly realistic images, concerns about potential misuse continue to grow. Supervised detection relies on large, curated datasets and struggles to generalize across diverse generators. In this work, we investigate the use of pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit some task-specific reasoning and chain-of-thought prompting offers gains, we show that task-aligned prompting elicits more focused reasoning and significantly improves performance without fine-tuning. Specifically, prefixing the model's response with the phrase "Let's examine the style and the synthesis artifacts" — a method we call zero-shot-s² — boosts Macro F1 scores by 8%–29%. These gains are consistent for two widely used open-source models and across three recent, diverse datasets spanning human faces, objects, and animals with images generated by 16 different models — demonstrating strong generalization.}
}

@inproceedings{kachwala2024heuristics,
  title={Advanced Heuristics for LLM Decoding Improve Chain-of-Thought Reasoning},
  author={Zoher Kachwala and Filippo Menczer},
  booktitle={In Progress},
  year={2024},
  status={In Progress},
  preview={decoding_heuristics.png},
  selected={true},
  bibtex_show={true},
  abstract={Proposes novel decoding strategies guided by answer labels to improve chain-of-thought reasoning in LLMs, boosting interpretability and task performance in complex generation scenarios.}
}

@inproceedings{kachwala2024moderation,
  title={Fine-Tuning Specialized LLMs for Large-Scale Community Content Moderation},
  author={Zoher Kachwala and Jisun An and Haewoon Kwak and Filippo Menczer},
  booktitle={In Progress},
  year={2024},
  status={In Progress},
  preview={community_moderation.png},
  selected={true},
  bibtex_show={true},
  abstract={Building a real-world moderation system by developing a framework to fine-tune and deploy specialized LLMs; this work supports healthier online communities by predicting nuanced rule violations across 500+ subreddits.}
}

@inproceedings{kachwala2024rematch,
  title={REMATCH: Robust and Efficient Knowledge Graph Matching},
  author={Zoher Kachwala and Jisun An and Haewoon Kwak and Filippo Menczer},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  year={2024},
  code={https://github.com/Zoher15/Rematch-RARE/tree/main},
  pdf={2024.findings-naacl.64.pdf},
  url={https://aclanthology.org/2024.findings-naacl.64},
  preview={rematchflow.png},
  selected={true},
  bibtex_show={true},
  abstract={Introduced a novel AMR similarity metric (rematch) that is 5x faster than SOTA and improves semantic similarity by up to 5%, alongside a new benchmark (RARE) for evaluating structural similarity in knowledge graphs.}
}

@inproceedings{aiyappa2023multi,
  title={A multi-platform collection of social media posts about the 2022 US midterm elections},
  author={Aiyappa, Rachith and DeVerna, Matthew R and Pote, Manita and Truong, Bao Tran and Zhao, Wanying and Axelrod, David and Pessianzadeh, Aria and Kachwala, Zoher and Kim, Munjung and Seckin, Ozgur Can and others},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={17},
  pages={981--989},
  year={2023},
  doi={10.1609/icwsm.v17i1.22205},
  abbr={ICWSM},
  bibtex_show={true},
  code={https://github.com/osome-iu/MEIU22},
  pdf={meiu.pdf},
  url={https://ojs.aaai.org/index.php/ICWSM/article/view/22205},
  preview={multiplatform.png},
  selected={true},
  abstract={Social media are utilized by millions of citizens to discuss important political issues. Politicians use these platforms to connect with the public and broadcast policy positions. Therefore, data from social media has enabled many studies of political discussion. While most analyses are limited to data from individual platforms, people are embedded in a larger information ecosystem spanning multiple social networks. Here we describe and provide access to the Indiana University 2022 U.S. Midterms Multi-Platform Social Media Dataset (MEIU22), a collection of social media posts from Twitter, Facebook, Instagram, Reddit, and 4chan. MEIU22 links to posts about the midterm elections based on a comprehensive list of keywords and tracks the social media accounts of 1,011 candidates from October 1 to December 25, 2022. We also publish the source code of our pipeline to enable similar multi-platform research projects.}
}

@article{aiyappa_inexplicable_2023,
	title = {The {Inexplicable} {Efficacy} of {Language} {Models}},
	volume = {29},
	issn = {1528-4972},
	url = {https://dl.acm.org/doi/10.1145/3589654},
	doi = {10.1145/3589654},
	number = {3},
	urldate = {2024-03-18},
	journal = {XRDS: Crossroads, The ACM Magazine for Students},
	author = {Aiyappa, Rachith and Kachwala, Zoher},
	month = apr,
	year = {2023},
	pages = {60--62},
  bibtex_show={true},
  selected={true},
  abstract={A brief insight into language modelling -- a statistical way of teaching language to machines, and its inexplicable capablities. It also highlights a few shortcomings of such models and discusses some approacheds to address them.}
}